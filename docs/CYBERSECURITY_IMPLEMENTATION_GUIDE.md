# Causal TGAN for Cybersecurity Network Attack Data: Implementation Guide

## Executive Summary

This document provides a comprehensive analysis of the Causal TGAN codebase and outlines the strategy for applying it to generate synthetic network attack data for cybersecurity applications. The goal is to generate high-quality synthetic tabular network traffic data for attacks using the NSL-KDD and CICIDS-2017 datasets.

---

## Table of Contents

1. [Background and Motivation](#background-and-motivation)
2. [Papers Review Summary](#papers-review-summary)
3. [Causal TGAN Methodology](#causal-tgan-methodology)
4. [Codebase Architecture Analysis](#codebase-architecture-analysis)
5. [Current Implementation Details](#current-implementation-details)
6. [Cybersecurity Application Strategy](#cybersecurity-application-strategy)
7. [Implementation Roadmap](#implementation-roadmap)
8. [Expected Outcomes](#expected-outcomes)
9. [References](#references)

---

## 1. Background and Motivation

### Why Synthetic Data for Cybersecurity?

**Key Challenges:**
- **Privacy concerns**: Real network data contains sensitive information
- **Data scarcity**: Limited labeled attack samples for training IDS/IPS systems
- **Class imbalance**: Normal traffic vastly outnumbers attack instances
- **Diversity**: Need varied attack patterns for robust ML model training
- **Regulatory compliance**: GDPR, HIPAA restrictions on sharing real network logs

**Benefits of Synthetic Data:**
- Privacy-preserving data sharing between organizations
- Balanced datasets for training anomaly detection systems
- Testing security systems without exposing real infrastructure
- Augmenting rare attack types (U2R, R2L)
- Accelerating security research and education

---

## 2. Papers Review Summary

### Paper 1: Causal-TGAN: Modeling Tabular Data Using Causally-Aware GAN

**Key Contributions:**
- Introduces Structural Causal Models (SCMs) into GAN architecture
- Uses Directed Acyclic Graphs (DAGs) to model causal relationships
- Hybrid architecture: Causal generators + Conditional GAN
- Handles three knowledge scenarios: full, partial, and no causal knowledge

**Technical Details:**
- **SCM Formulation**: `<X, F, U>` where:
  - X = Endogenous variables (observed features)
  - F = Causal mechanisms (neural networks)
  - U = Exogenous variables (random noise)
- **Encoding**: Mode-specific normalization for continuous, one-hot for discrete
- **Architecture**: Each variable has its own generator conditioned on parents

### Paper 2: A Review of Generative Models in Generating Synthetic Attack Data for Cybersecurity

**Key Findings:**
- Comprehensive review of 13+ synthetic data generation methods
- Applications: IDS training, anomaly detection, penetration testing
- **Challenges in cybersecurity**:
  - Maintaining attack signatures and patterns
  - Preserving temporal dependencies in network flows
  - Handling extreme class imbalance
  - Ensuring statistical fidelity and privacy

**Method Comparison:**
- **Statistical**: ROS, SMOTE, ADASYN (limited novelty)
- **Probabilistic**: GMM, Bayesian Networks (complex dependencies)
- **Deep Learning**: TVAE, Diffusion Models
- **GANs**: CTGAN, CopulaGAN, GANBLR++, CastGAN, Causal-TGAN

### Paper 3: Synthetic Data Generation in Cybersecurity: A Comparative Analysis

**Experimental Results on NSL-KDD:**
- **Dataset**: 125,973 instances, 42 columns
- **Feature Selection**: Mutual Information → top 25% → 26 features
- **Best Performers**:
  - CopulaGAN: 98% accuracy
  - CTGAN: 97% accuracy
  - SMOTE: 99% (but no novelty, just oversampling)

**Experimental Results on CICIDS-2017:**
- **Dataset**: 2,830,743 instances, 79 columns
- **Feature Selection**: Mutual Information → 21 features
- **Best Performers**:
  - CopulaGAN: 98% accuracy
  - CTGAN: 96% accuracy

**Evaluation Metrics:**
1. **Fidelity**: Statistical similarity (correlations, distributions)
2. **Utility**: ML performance (TRTR vs TSTR accuracy)
3. **Class Balance**: Distribution of normal vs attack instances

**Attack Types in NSL-KDD:**
- DoS (Denial of Service)
- Probe (Scanning/Reconnaissance)
- U2R (User to Root privilege escalation)
- R2L (Remote to Local unauthorized access)

---

## 3. Causal TGAN Methodology

### Structural Causal Models (SCMs)

**Definition**: A triplet `<X, F, U>`

```
X_i = f_i(PA_i, U_i)
```

Where:
- `X_i`: Observable variable (e.g., protocol type, packet size)
- `PA_i`: Parent variables in the causal graph
- `f_i`: Causal mechanism (neural network)
- `U_i`: Exogenous noise (standard normal distribution)

### Three Knowledge Scenarios

1. **Full Knowledge**: Complete causal graph known
   - All variables generated by causal generators
   - Best performance, requires domain expertise

2. **Partial Knowledge**: Some causal relationships known
   - Hybrid: Causal generators for known + Conditional GAN for unknown
   - Balances performance and practicality

3. **No Knowledge**: No causal graph available
   - Falls back to Conditional GAN only
   - Similar to CTGAN

### Why Causal TGAN for Cybersecurity?

**Advantages:**
1. **Preserves Attack Logic**: Causal relationships encode attack semantics
   - Example: `port_scan → multiple_connections → probe_attack`
   - Example: `failed_login → su_attempted → U2R_attack`

2. **Better Generalization**: Learns mechanisms, not just correlations
   - Can generate novel attack variants
   - More robust to distribution shifts

3. **Interpretability**: Causal graph is human-readable
   - Security analysts can validate attack patterns
   - Explainable synthetic data generation

4. **Handles Imbalance**: Training-by-sampling mechanism
   - Can oversample rare attacks without losing quality

---

## 4. Codebase Architecture Analysis

### Directory Structure

```
CausalTGAN/
├── configuration.py          # Config classes for training and models
├── dataset.py               # Data transformers and loaders
├── train.py                 # Main training script
├── sampling.py              # Synthetic data generation script
├── helper/
│   ├── constant.py          # Dataset-specific column definitions
│   ├── feature_info.py      # Feature metadata management
│   ├── graphFromTetrad.py   # Causal graph parsing from Tetrad
│   ├── trainer.py           # Training utilities
│   └── utils.py             # Helper functions (data loading, graph parsing)
├── model/
│   ├── causalTGAN.py        # Main Causal-TGAN orchestrator
│   ├── condGAN.py           # Conditional GAN for partial knowledge
│   └── module/
│       ├── generator.py      # Causal generators (continuous/categorical)
│       └── discriminator.py  # Discriminator networks
└── data/
    └── real_world/
        └── adult/
            ├── train.csv
            ├── test.csv
            └── graph.txt     # Pickled causal graph structure
```

### Key Components

#### 1. Configuration Classes (`configuration.py`)

```python
# Causal-TGAN configuration
CausalTGANConfig(
    causal_graph,  # List of [node, parents] pairs
    z_dim,         # Exogenous noise dimension
    pac_num,       # Samples per PAC (Packing)
    D_iter         # Discriminator iterations per generator step
)

# Conditional GAN configuration (for partial/no knowledge)
CondGANConfig(
    causal_graph, col_names, col_dims,
    z_dim=128, pac_num=10, D_iter=5
)

# Training options
TrainingOptions(
    batch_size, number_of_epochs,
    runs_folder, experiment_name
)
```

#### 2. Data Transformers (`dataset.py`)

**Three Transformer Types:**

1. **DataTransformer (CTGAN-style)**:
   - Continuous: Bayesian GMM with mode-specific normalization
   - Discrete: One-hot encoding
   - Activation: `tanh` for continuous, `softmax` for discrete

2. **GeneralTransformer**:
   - Continuous: Standard scaling (mean=0, std=1)
   - Discrete: One-hot encoding
   - Activation: `tanh` for continuous, `softmax` for discrete

3. **PlainTransformer**:
   - Continuous: Standard scaling
   - Discrete: Label encoding (integer labels)
   - Activation: `tanh` for continuous, `relu` for discrete

**Selection**: CTGAN-style transformer recommended for best quality

#### 3. Causal Graph Structure

**Format** (stored as pickled Python list):

```python
causal_graph = [
    ['age', []],                          # Root node, no parents
    ['workclass', ['age', 'marital-status', 'education', 'native-country']],
    ['fnlwgt', []],
    ['education', ['workclass', 'race', 'sex', 'native-country']],
    ['education-num', ['education']],
    # ... more nodes ...
    ['label', ['workclass', 'race', 'native-country', ...]]  # Target
]
```

**Requirements:**
- List of `[node_name, parent_list]` pairs
- Node order must match DataFrame column order
- Must be a valid DAG (no cycles)
- Parent names must be valid column names

#### 4. Generator Architecture (`model/module/generator.py`)

**Causal Generator** (`causal_generator`):
- Creates a `CausalNode` for each variable in the graph
- Each node has its own neural network generator
- Sampling is autoregressive following topological order

**CausalNode** (per-variable generator):
```python
Input: concat([parent_values, exogenous_noise])
├── Linear(parent_dim + z_dim, 64)
├── BatchNorm1d + LeakyReLU
├── Linear(64, 128)
├── BatchNorm1d + LeakyReLU
├── Linear(128, 128)
├── BatchNorm1d + LeakyReLU
└── Linear(128, feature_dim)
    └── Activation: tanh (continuous) or gumbel_softmax (categorical)
```

**Conditional GAN Generator** (for unknown variables):
```python
Input: concat([noise, known_variables])  # z_dim=128
├── Residual(128, 256)
├── Residual(128+256, 256)
└── Linear(128+256+256, data_dim)
    └── Apply activations per column type
```

#### 5. Discriminator Architecture

**Causal-TGAN Discriminator**:
- Simple MLP for Wasserstein GAN with gradient penalty
- Input: Full data vector (all variables concatenated)

**Conditional GAN Discriminator**:
- Similar architecture with PAC (Packing) support
- Helps with mode collapse

#### 6. Training Workflow (`model/causalTGAN.py`)

**Three Training Modes**:

1. **Full Knowledge** (`train_full_knowledge`):
   ```python
   for epoch in epochs:
       for batch in data:
           # Train discriminator
           real_data = batch
           fake_data = causal_controller.sample(batch_size)
           D_loss = D(fake) - D(real) + gradient_penalty

           # Train generators (all causal mechanisms)
           if step % D_iter == 0:
               fake_data = causal_controller.sample(batch_size)
               G_loss = -D(fake)
   ```

2. **Partial Knowledge** (`train_partial_knowledge`):
   - Train Causal-TGAN on known variables first
   - Train Conditional GAN on remaining variables (conditioned on known)
   - Both models saved separately

3. **No Knowledge** (`train_no_knowledge`):
   - Only train Conditional GAN (no causal component)
   - Equivalent to CTGAN

**Gradient Penalty**: Wasserstein GAN-GP for stable training

**PAC (Packing)**: Multiple samples processed together to reduce mode collapse

#### 7. Sampling (`sampling.py`)

**Workflow**:
```python
1. Load trained model from checkpoint
2. Set generators to eval mode
3. Generate samples in batches (1000 at a time)
4. Inverse transform to original data space
5. Save to CSV
```

**Hybrid Sampling** (partial knowledge):
- First sample from causal generators
- Use those as conditions for Conditional GAN
- Merge results into complete samples

---

## 5. Current Implementation Details

### Data Loading (`helper/utils.py`)

**Function**: `load_data_graph(data_name, graph_path=None)`

**Process**:
1. Determine data folder path from dataset name
2. Load causal graph from `graph.txt` (pickled list)
3. Load training data from `train.csv`
4. Identify discrete columns based on dataset type or predefined constants
5. Return: `(data, col_names, discrete_cols, causal_graph)`

**Discrete Column Definitions** (`helper/constant.py`):
```python
ADULT_CATEGORY = ['workclass', 'education', 'marital-status', ...]
CABS_CATEGORY = ['Type_of_Cab', 'Gender', ...]
LOAN_CATEGORY = ['label', 'Term', 'Home Ownership', 'Purpose']
# ... etc ...
```

**Note**: No cybersecurity datasets defined yet (NSL-KDD, CICIDS-17)

### Feature Information (`helper/feature_info.py`)

**FeatureINFO Class**:
- Stores type information (continuous/discrete) for each feature
- Stores dimensionality after transformation (e.g., one-hot expansion)
- Stores position indices in the transformed data matrix

**Example**:
```python
feature_info = FeatureINFO(
    feature_names=['protocol', 'service', 'duration'],
    discrete_cols=['protocol', 'service'],
    feature_dims=[3, 70, 1]  # protocol has 3 types, service has 70, duration is 1D
)

# Query positions
protocol_pos = feature_info.get_position_by_name('protocol')  # [0, 1, 2]
service_pos = feature_info.get_position_by_name('service')    # [3, 4, ..., 72]
duration_pos = feature_info.get_position_by_name('duration')  # [73]
```

### Training Scripts (`train.py`)

**Command-line Arguments**:
```bash
python train.py \
    --data_name adult \
    --device_idx 0 \
    --batch_size 500 \
    --epochs 400 \
    --pac_num 1 \
    --z_dim 2 \
    --d_iter 3 \
    --transformer_type ctgan \
    --runs_folder ./Testing
```

**Workflow**:
1. Load data and causal graph
2. Transform data using specified transformer
3. Create `FeatureINFO` object
4. Determine knowledge scenario (full/partial/no)
5. Initialize `CausalTGAN` model
6. Train using appropriate trainer function
7. Save checkpoints, config, transformer, and feature_info

---

## 6. Cybersecurity Application Strategy

### Target Datasets

#### NSL-KDD Dataset

**Statistics**:
- **Size**: 125,973 instances
- **Original Features**: 42 (41 features + 1 label)
- **After Feature Selection**: 26 features
- **Attack Types**: DoS, Probe, U2R, R2L, Normal

**Key Features** (selected by Mutual Information):
- `src_bytes`, `dst_bytes`: Data transfer volumes
- `same_srv_rate`, `diff_srv_rate`: Service access patterns
- `serror_rate`, `rerror_rate`: Error rates
- `dst_host_same_srv_rate`: Host behavior patterns
- `protocol_type`, `service`, `flag`: Protocol information
- `logged_in`, `root_shell`, `su_attempted`: Authentication/privilege flags

**Discrete Columns**:
```python
NSL_KDD_CATEGORY = ['protocol_type', 'service', 'flag', 'land',
                    'logged_in', 'root_shell', 'su_attempted',
                    'is_hot_login', 'is_guest_login', 'label']
```

#### CICIDS-2017 Dataset

**Statistics**:
- **Size**: 2,830,743 instances
- **Original Features**: 79
- **After Feature Selection**: 21 features
- **Attack Types**: Benign, DoS, DDoS, Port Scan, Brute Force, Web Attack, Infiltration, Botnet

**Key Features** (selected by Mutual Information):
- `Average Packet Size`, `Packet Length Std`, `Packet Length Mean`
- `Flow Duration`, `Flow IAT Mean`, `Flow IAT Std`
- `Fwd/Bwd Packet Length Max/Min/Mean`
- `Active Mean`, `Idle Mean`

**Discrete Columns** (likely):
```python
CICIDS_CATEGORY = ['label']  # Primarily continuous features
```

### Causal Relationships in Network Traffic

#### Known Causal Dependencies

**Protocol-Level**:
```
protocol_type → service → dst_port
```
- TCP/UDP determines available services
- Service determines destination port

**Behavior Patterns**:
```
logged_in → num_file_creations
logged_in → num_shell_prompts
logged_in → root_shell
```
- Authentication enables certain actions

**Attack Signatures**:
```
su_attempted → num_failed_logins → U2R_attack
count → srv_count → DoS_attack
dst_host_srv_count → dst_host_same_srv_rate → Probe_attack
```

**Error Cascades**:
```
flag → serror_rate → rerror_rate
```
- Connection state affects error patterns

#### Example Causal Graph for NSL-KDD (Partial Knowledge)

```python
nsl_kdd_causal_graph = [
    # Protocol layer
    ['protocol_type', []],
    ['service', ['protocol_type']],
    ['flag', ['protocol_type', 'service']],

    # Connection characteristics
    ['duration', []],
    ['src_bytes', ['protocol_type', 'service']],
    ['dst_bytes', ['protocol_type', 'service', 'src_bytes']],

    # Authentication and access
    ['logged_in', ['service', 'flag']],
    ['root_shell', ['logged_in']],
    ['su_attempted', ['logged_in']],

    # Error rates
    ['serror_rate', ['flag', 'protocol_type']],
    ['rerror_rate', ['flag', 'serror_rate']],

    # Service patterns
    ['same_srv_rate', ['service']],
    ['diff_srv_rate', ['service']],

    # Attack label (depends on everything)
    ['label', ['protocol_type', 'flag', 'logged_in', 'root_shell',
               'su_attempted', 'serror_rate', 'same_srv_rate',
               'dst_host_same_srv_rate']]
]
```

**Strategy**: Partial knowledge approach
- Define causal relationships for ~15-20 key features
- Use Conditional GAN for remaining features
- Balances domain knowledge with flexibility

### Feature Selection Pipeline

**Implementation** (using Mutual Information):

```python
from sklearn.feature_selection import mutual_info_classif

# Calculate MI scores
mi_scores = mutual_info_classif(X, y, random_state=42)

# Select top 25% features
threshold = np.percentile(mi_scores, 75)
selected_features = X.columns[mi_scores >= threshold]

# For NSL-KDD: 42 → 26 features
# For CICIDS-17: 79 → 21 features
```

**Benefits**:
- Reduces dimensionality
- Removes redundant/irrelevant features
- Improves GAN training stability
- Faster generation

---

## 7. Implementation Roadmap

### Phase 1: Data Preparation

**Task 1.1**: Download and preprocess NSL-KDD
```python
# Directory structure
data/real_world/nsl_kdd/
├── train.csv      # Training data
├── test.csv       # Test data
└── graph.txt      # Causal graph (to be created)
```

**Task 1.2**: Implement feature selection
```python
# Create script: preprocess_nslkdd.py
1. Load NSL-KDD data
2. Apply Mutual Information feature selection (top 25%)
3. Create train/test splits
4. Save preprocessed data
```

**Task 1.3**: Define discrete columns in `constant.py`
```python
# Add to helper/constant.py
NSL_KDD_CATEGORY = ['protocol_type', 'service', 'flag', 'land',
                    'logged_in', 'root_shell', 'su_attempted',
                    'is_hot_login', 'is_guest_login', 'label']
```

**Task 1.4**: Update `utils.py` to recognize NSL-KDD
```python
# In get_discrete_cols()
if data_name == 'nsl_kdd':
    discrete_cols = NSL_KDD_CATEGORY
```

### Phase 2: Causal Graph Construction

**Task 2.1**: Domain expert consultation
- Interview cybersecurity experts
- Review attack taxonomy literature
- Identify key causal mechanisms (protocol → service, login → privilege)

**Task 2.2**: Create initial causal graph
```python
# Create script: create_nslkdd_graph.py
nsl_kdd_graph = [
    ['protocol_type', []],
    ['service', ['protocol_type']],
    # ... (see example above)
    ['label', [...]]  # Target variable
]

# Save as pickle
import pickle
with open('data/real_world/nsl_kdd/graph.txt', 'wb') as f:
    pickle.dump(nsl_kdd_graph, f)
```

**Task 2.3**: Validate graph structure
```python
# Check for cycles
from CausalTGAN.helper.utils import _no_cycle, read_amat

# Convert to adjacency matrix and validate
assert _no_cycle(adjacency_matrix), "Graph contains cycles!"
```

**Task 2.4**: Alternative - Learn graph from data
```python
# Use Tetrad or other causal discovery tools
# - PC algorithm
# - GES (Greedy Equivalence Search)
# - FCI (Fast Causal Inference)

# Convert Tetrad output to CausalTGAN format
from CausalTGAN.helper.graphFromTetrad import *
```

### Phase 3: Model Configuration and Training

**Task 3.1**: Configure model hyperparameters
```python
# Recommended settings for cybersecurity data
CausalTGANConfig(
    causal_graph=nsl_kdd_graph,
    z_dim=2,           # Exogenous noise dimension
    pac_num=1,         # PAC for stability
    D_iter=3           # Discriminator steps per generator
)

CondGANConfig(
    causal_graph=nsl_kdd_graph,
    col_names=nsl_kdd_columns,
    col_dims=nsl_kdd_dims,
    z_dim=128,
    pac_num=10,
    D_iter=5
)

TrainingOptions(
    batch_size=500,
    number_of_epochs=400,
    runs_folder='./experiments/nsl_kdd',
    experiment_name='CausalTGAN_NSLKDD'
)
```

**Task 3.2**: Train Causal-TGAN model
```bash
# Command
python train.py \
    --data_name nsl_kdd \
    --device_idx 0 \
    --batch_size 500 \
    --epochs 400 \
    --pac_num 1 \
    --z_dim 2 \
    --d_iter 3 \
    --transformer_type ctgan \
    --runs_folder ./experiments
```

**Task 3.3**: Monitor training
- Track generator and discriminator losses
- Check for mode collapse
- Validate sample quality during training

### Phase 4: Synthetic Data Generation

**Task 4.1**: Generate synthetic samples
```python
# Using sampling.py
from CausalTGAN.sampling import synthetic

model_path = './experiments/CausalTGAN_NSLKDD...'
gen_num = 100000  # Generate 100k samples
device = torch.device('cuda:0')

synthetic(model_path, gen_num, device,
          save_path='./synthetic_nslkdd.csv')
```

**Task 4.2**: Generate balanced dataset
```python
# Modify sampling to oversample rare attacks
# Generate per-class:
# - Normal: 50,000
# - DoS: 20,000
# - Probe: 15,000
# - U2R: 10,000 (rare!)
# - R2L: 5,000 (rare!)
```

### Phase 5: Evaluation

**Task 5.1**: Fidelity Metrics
```python
from scipy.stats import ks_2samp
from sklearn.metrics import mean_absolute_error

# 1. Univariate distributions (KS test)
for col in columns:
    stat, pval = ks_2samp(real[col], synthetic[col])

# 2. Correlation preservation
real_corr = real.corr()
synth_corr = synthetic.corr()
corr_mae = mean_absolute_error(real_corr, synth_corr)

# 3. Principal Component Analysis
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
real_pca = pca.fit_transform(real)
synth_pca = pca.transform(synthetic)
# Visual comparison
```

**Task 5.2**: Utility Metrics
```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score

# TRTR (Train Real, Test Real)
model = RandomForestClassifier()
model.fit(real_train, y_train)
trtr_acc = model.score(real_test, y_test)

# TSTR (Train Synthetic, Test Real)
model = RandomForestClassifier()
model.fit(synthetic_train, y_synth_train)
tstr_acc = model.score(real_test, y_test)

# Utility = TSTR / TRTR (should be > 0.95)
```

**Task 5.3**: Class Balance Metrics
```python
import pandas as pd

real_dist = real['label'].value_counts(normalize=True)
synth_dist = synthetic['label'].value_counts(normalize=True)

# Compare distributions
balance_diff = (synth_dist - real_dist).abs().mean()
```

**Task 5.4**: Domain-Specific Validation
```python
# 1. Check attack signatures
# DoS: high src_bytes, low dst_bytes, high count
dos_samples = synthetic[synthetic['label'] == 'dos']
assert dos_samples['count'].mean() > threshold

# 2. Check protocol constraints
# HTTP service should have TCP protocol
http_samples = synthetic[synthetic['service'] == 'http']
assert (http_samples['protocol_type'] == 'tcp').all()

# 3. Check logical constraints
# root_shell requires logged_in
root_samples = synthetic[synthetic['root_shell'] == 1]
assert (root_samples['logged_in'] == 1).all()
```

### Phase 6: Baseline Comparison

**Task 6.1**: Implement baseline methods
```python
# 1. SMOTE
from imblearn.over_sampling import SMOTE
smote = SMOTE()
X_smote, y_smote = smote.fit_resample(X, y)

# 2. CTGAN (no causal knowledge)
# Train with causal_graph = []

# 3. CopulaGAN
from sdv.tabular import CopulaGAN
model = CopulaGAN()
model.fit(data)
synthetic = model.sample(100000)
```

**Task 6.2**: Run comparative evaluation
```python
methods = ['Causal-TGAN', 'CTGAN', 'CopulaGAN', 'SMOTE']
results = {}

for method in methods:
    results[method] = {
        'fidelity': compute_fidelity(synthetic),
        'utility_trtr': trtr_accuracy,
        'utility_tstr': tstr_accuracy,
        'class_balance': compute_balance(synthetic)
    }

# Create comparison table
import pandas as pd
df_results = pd.DataFrame(results).T
```

**Expected Results** (based on papers):
```
Method          | TSTR Acc | Fidelity | Balance
----------------|----------|----------|--------
Causal-TGAN     | 97-98%   | High     | Good
CTGAN           | 96-97%   | High     | Good
CopulaGAN       | 98%      | High     | Good
SMOTE           | 99%      | Low      | Perfect
```

---

## 8. Expected Outcomes

### Advantages of Causal-TGAN for Cybersecurity

1. **Attack Logic Preservation**
   - Causal graph encodes attack semantics
   - Generated samples follow realistic attack patterns
   - Better than correlation-based methods

2. **Novelty and Diversity**
   - Not just interpolation like SMOTE
   - Can generate new attack variants
   - Helps ML models generalize better

3. **Interpretability**
   - Causal graph is human-readable
   - Security analysts can validate relationships
   - Transparency in synthetic data generation

4. **Handling Rare Attacks**
   - Can oversample U2R and R2L attacks
   - Maintains attack signatures even when rare
   - Better than pure GAN methods

5. **Privacy Preservation**
   - No direct copying of real samples
   - Causal mechanisms learned, not memorized
   - Suitable for sharing between organizations

### Potential Challenges

1. **Causal Graph Construction**
   - Requires domain expertise
   - Time-consuming manual process
   - Automated discovery may introduce errors

2. **Computational Cost**
   - More complex than CTGAN (multiple generators)
   - Longer training time
   - Requires GPU for large datasets

3. **Hyperparameter Tuning**
   - z_dim, pac_num, D_iter require experimentation
   - Different datasets may need different settings

4. **Evaluation Complexity**
   - Need multiple metrics (fidelity + utility + domain-specific)
   - Trade-offs between metrics

### Success Criteria

**Minimum Requirements**:
- TSTR accuracy ≥ 95% (compared to TRTR)
- Correlation MAE ≤ 0.1
- Attack signatures preserved (domain validation)
- Class balance achieved for rare attacks

**Stretch Goals**:
- Outperform CTGAN and CopulaGAN in utility
- Generate novel attack patterns (verified by experts)
- Publish results in cybersecurity conference/journal

---

## 9. References

### Papers

1. **Causal-TGAN: Modeling Tabular Data Using Causally-Aware GAN**
   - File: `34_causal_tgan_modeling_tabular_d.pdf`
   - Key contribution: Causal TGAN methodology

2. **A Review of Generative Models in Generating Synthetic Attack Data for Cybersecurity**
   - File: `electronics-13-00322.pdf`
   - Key contribution: Comprehensive review of methods

3. **Synthetic Data Generation in Cybersecurity: A Comparative Analysis**
   - File: `2410.16326v1.pdf`
   - Key contribution: Experimental results on NSL-KDD and CICIDS-17

### Datasets

- **NSL-KDD**: https://www.unb.ca/cic/datasets/nsl.html
- **CICIDS-2017**: https://www.unb.ca/cic/datasets/ids-2017.html

### Related Tools

- **Tetrad**: Causal discovery tool (http://www.phil.cmu.edu/tetrad/)
- **SDV**: Synthetic Data Vault (includes CTGAN, CopulaGAN)
- **SMOTE**: Imbalanced-learn library

---

## Appendix A: Code Modifications Checklist

### Files to Create

- [ ] `data/real_world/nsl_kdd/train.csv`
- [ ] `data/real_world/nsl_kdd/test.csv`
- [ ] `data/real_world/nsl_kdd/graph.txt`
- [ ] `preprocess_nslkdd.py` (feature selection script)
- [ ] `create_nslkdd_graph.py` (causal graph creation)
- [ ] `evaluate_nslkdd.py` (evaluation metrics)

### Files to Modify

- [ ] `helper/constant.py` - Add `NSL_KDD_CATEGORY`
- [ ] `helper/utils.py` - Add NSL-KDD to `get_discrete_cols()`
- [ ] `helper/utils.py` - Add NSL-KDD to `path_by_name()` if needed

### Files to Use As-Is

- [x] `configuration.py`
- [x] `dataset.py`
- [x] `train.py`
- [x] `sampling.py`
- [x] `model/causalTGAN.py`
- [x] `model/condGAN.py`
- [x] All files in `model/module/`

---

## Appendix B: Example Causal Graphs

### Option 1: Minimal Graph (Conservative)

```python
nsl_kdd_minimal = [
    ['protocol_type', []],
    ['service', ['protocol_type']],
    ['logged_in', ['service']],
    ['root_shell', ['logged_in']],
    ['label', ['protocol_type', 'service', 'logged_in', 'root_shell']]
]
```
- Only 5 variables with causal relationships
- Rest handled by Conditional GAN
- Lower risk of incorrect causal assumptions

### Option 2: Moderate Graph (Balanced)

```python
nsl_kdd_moderate = [
    # Protocol layer
    ['protocol_type', []],
    ['service', ['protocol_type']],
    ['flag', ['protocol_type']],

    # Traffic volume
    ['src_bytes', []],
    ['dst_bytes', ['src_bytes']],

    # Authentication
    ['logged_in', ['service', 'flag']],
    ['root_shell', ['logged_in']],
    ['su_attempted', ['logged_in']],

    # Errors
    ['serror_rate', ['flag']],
    ['rerror_rate', ['flag']],

    # Label
    ['label', ['protocol_type', 'flag', 'logged_in', 'root_shell',
               'serror_rate']]
]
```
- 11 variables with causal relationships
- Balance between causal and GAN components
- Recommended starting point

### Option 3: Comprehensive Graph (Ambitious)

```python
nsl_kdd_comprehensive = [
    # Full graph with all 26 selected features
    # Detailed causal relationships for all variables
    # Higher potential reward but risk of incorrect assumptions
    # See Phase 2 for detailed construction
]
```
- All 26 features with causal relationships
- Maximum leverage of causal structure
- Requires extensive domain expertise

---

## Appendix C: Training Tips

### GPU Memory Optimization

```python
# If out of memory, reduce:
--batch_size 256      # Instead of 500
--pac_num 1          # Reduce packing
--z_dim 2            # Smaller noise dimension

# Monitor GPU usage
nvidia-smi -l 1
```

### Training Stability

```python
# If discriminator dominates (D_loss → 0):
--d_iter 1           # Fewer discriminator steps

# If generator dominates (G_loss → 0):
--d_iter 5           # More discriminator steps

# If mode collapse occurs:
--pac_num 10         # Increase packing
```

### Data Preprocessing

```python
# Handle missing values
data.fillna(data.median(), inplace=True)  # For continuous
data.fillna(data.mode().iloc[0], inplace=True)  # For categorical

# Remove outliers (optional)
from scipy.stats import zscore
z_scores = zscore(data[continuous_cols])
data = data[(np.abs(z_scores) < 3).all(axis=1)]

# Normalize labels
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
data['label'] = le.fit_transform(data['label'])
```

---

## Document Version

- **Version**: 1.0
- **Date**: 2025-11-09
- **Author**: Implementation Guide for Causal TGAN Cybersecurity Research
- **Status**: Initial Draft

---

**END OF DOCUMENT**
